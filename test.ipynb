{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a5d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm # Loop progress bar\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('png')\n",
    "from torchview import draw_graph\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4409583",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816db4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keyword - missing:\", train_df['keyword'].isna().sum(), f\"({train_df['keyword'].isna().mean()*100:.1f}%)\")\n",
    "print(\"Keyword - unique:\", train_df['keyword'].nunique())\n",
    "print(\"\\nLocation - missing:\", train_df['location'].isna().sum(), f\"({train_df['location'].isna().mean()*100:.1f}%)\")\n",
    "print(\"Location - unique:\", train_df['location'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['target'].tolist()\n",
    "        self.text = df['text'].tolist()\n",
    "\n",
    "        # Tokenizer\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.tokens = [self.tokenizer(t) for t in self.text]\n",
    "\n",
    "        # Build vocab pour vectorization\n",
    "        counter = Counter()\n",
    "        for tokens in self.tokens:\n",
    "            counter.update(tokens)\n",
    "        self.vocab = Vocab(counter, specials=['<unk>', '<pad>'], min_freq=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens[idx]\n",
    "        indices = [self.vocab[token] for token in tokens]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def print_vocab(self):\n",
    "        print(\"Vocab size:\", len(self.vocab))\n",
    "        print(\"Top tokens:\", self.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TweetsDataset(train_df)\n",
    "len(dataset)\n",
    "train_set, val_set = random_split(dataset, [.85, .15])\n",
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.text[1])\n",
    "print(dataset.tokens[1])\n",
    "print(indices := [dataset.vocab[token] for token in dataset.tokens[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, hidden_size=8):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.hidden = nn.Linear(embed_dim, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "        self.hidden.weight.data.uniform_(-initrange, initrange)\n",
    "        self.hidden.bias.data.zero_()\n",
    "\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = self.hidden(embedded)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47278b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_bag(batch: list[tuple[torch.Tensor, torch.Tensor]]):\n",
    "    text_list, label_list = [], []  # [(tokens_tensor, label),...]\n",
    "    for text, label in batch:\n",
    "        text_list.append(text)\n",
    "        label_list.append(label)\n",
    "\n",
    "    # offsets for EmbeddingBag askip ca marche mieux avec des fully connected (source: un LLM)\n",
    "    lengths = torch.tensor([t.size(0) for t in text_list], dtype=torch.long)\n",
    "    offsets = torch.cat((torch.tensor([0], dtype=torch.long), torch.cumsum(lengths, dim=0)[:-1]))\n",
    "    text = torch.cat(text_list, dim=0)\n",
    "    labels = torch.stack(label_list).long()\n",
    "\n",
    "    # print(text.shape, offsets.shape)\n",
    "    return text, offsets, labels\n",
    "\n",
    "\n",
    "def train_model(model, dataloaders, optimizer, criterion, N_EPOCHS = 10):\n",
    "    plotloss = PlotLosses(figsize=(16,6))\n",
    "    \n",
    "    for epoch in tqdm(range(1, N_EPOCHS + 1)):\n",
    "        model.train()\n",
    "        total_acc, total_count = 0, 0\n",
    "        # log_interval = 100\n",
    "\n",
    "        for idx, (text, offsets, labels) in enumerate(dataloaders[\"train\"]):\n",
    "            optimizer.zero_grad()\n",
    "            text, offsets, labels = text.to(device), offsets.to(device), labels.to(device)\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_acc += (output.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "            # if idx % log_interval == 0 and idx > 0:\n",
    "            #     # print(f'At batch {idx}, accuracy: {total_acc/total_count:.4f}')\n",
    "            #     total_acc, total_count = 0, 0\n",
    "        \n",
    "        plotloss.update({\n",
    "            'loss': loss.item(),\n",
    "            'accuracy': total_acc/total_count,\n",
    "            'val_accuracy': evaluate(model, dataloaders[\"validation\"], criterion)\n",
    "        })\n",
    "        plotloss.send()\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text, offsets, labels in iterator:\n",
    "            text, offsets, labels = text.to(device), offsets.to(device), labels.to(device)\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, labels)\n",
    "            total_acc += (output.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "    return total_acc / total_count\n",
    "\n",
    "\n",
    "def predict(model, iterator):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for text, offsets, labels in iterator:\n",
    "            text, offsets = text.to(device), offsets.to(device)\n",
    "            output = model(text, offsets)\n",
    "            preds = output.argmax(1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c97efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch_bag\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch_bag\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"validation\": val_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "LR = 0.01\n",
    "\n",
    "model = MLPModel(vocab_size=len(dataset.vocab), embed_dim=1, num_class=2, hidden_size=2).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_model(model, dataloaders, optimizer, criterion, N_EPOCHS=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab4f71",
   "metadata": {},
   "source": [
    "Ok le truc est globalement idiot mais c mieux que full random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4642ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first tweet embedding vector for first tweets\n",
    "\n",
    "text, offset, _ = next(iter(dataloaders[\"train\"]))\n",
    "embedding = model.embedding(text, offset).detach().cpu().numpy().squeeze()\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b59c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9344cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view embeddings in 2d space\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "MAX_POINTS = 50000\n",
    "\n",
    "def gaussian_from_embedding(model, set, max_points=1000):\n",
    "    false_list, true_list = [], []\n",
    "    i = 0\n",
    "\n",
    "    for text, offset, label in tqdm(dataloaders[set]):\n",
    "        vector = model.embedding(text, offset).detach().cpu().numpy().squeeze()\n",
    "        if label.item() == 0:\n",
    "            false_list.append(vector)\n",
    "        else:\n",
    "            true_list.append(vector)\n",
    "        i += 1\n",
    "        if i >= max_points:\n",
    "            break\n",
    "    \n",
    "    return gaussian_kde(false_list), gaussian_kde(true_list)\n",
    "\n",
    "\n",
    "xs = np.linspace(-0.5, 0.5, 200)\n",
    "\n",
    "# Dummy model for comparison\n",
    "dummy_model = MLPModel(vocab_size=len(dataset.vocab), embed_dim=1, num_class=2, hidden_size=2)\n",
    "\n",
    "train_false_density, train_true_density = gaussian_from_embedding(dummy_model, \"train\", max_points=MAX_POINTS)\n",
    "val_false_density, val_true_density = gaussian_from_embedding(dummy_model, \"validation\", max_points=MAX_POINTS)\n",
    "\n",
    "plt.suptitle(\"Tweet embeddings visualized in 2D space\")\n",
    "ax[0, 0].set_title(\"Untrained model - Train set\")\n",
    "ax[0, 0].plot(xs, train_false_density(xs), color='b', label='No Disaster')\n",
    "ax[0, 0].plot(xs, train_true_density(xs), color='r', label='Disaster')\n",
    "ax[0, 0].legend()\n",
    "ax[0, 1].set_title(\"Untrained model - Validation set\")\n",
    "ax[0, 1].plot(xs, val_false_density(xs), color='b', label='No Disaster')\n",
    "ax[0, 1].plot(xs, val_true_density(xs), color='r', label='Disaster')\n",
    "ax[0, 1].legend()\n",
    "\n",
    "# True model embeddings\n",
    "train_false_density, train_true_density = gaussian_from_embedding(model, \"train\", max_points=MAX_POINTS)\n",
    "val_false_density, val_true_density = gaussian_from_embedding(model, \"validation\", max_points=MAX_POINTS)\n",
    "\n",
    "plt.suptitle(\"Tweet embeddings visualized in 2D space\")\n",
    "ax[1, 0].set_title(\"Trained model - Train set\")\n",
    "ax[1, 0].plot(xs, train_false_density(xs), color='b', label='No Disaster')\n",
    "ax[1, 0].plot(xs, train_true_density(xs), color='r', label='Disaster')\n",
    "ax[1, 0].legend()\n",
    "ax[1, 1].set_title(\"Trained model - Validation set\")\n",
    "ax[1, 1].plot(xs, val_false_density(xs), color='b', label='No Disaster')\n",
    "ax[1, 1].plot(xs, val_true_density(xs), color='r', label='Disaster')\n",
    "ax[1, 1].legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a96e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56769cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "print_text, print_offsets, print_labels = next(iter(dataloaders[\"validation\"]))\n",
    "summary(\n",
    "    model,\n",
    "    input_data=[print_text, print_offsets],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = draw_graph(model, input_data=[print_text, print_offsets])\n",
    "graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, vocab, tokenizer):\n",
    "        self.text = df['text'].tolist()\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokens = [self.tokenizer(t) for t in self.text]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens[idx]\n",
    "        indices = [self.vocab[token] for token in tokens]\n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "test_dataset = TestDataset(test_df, dataset.vocab, dataset.tokenizer)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch_bag\n",
    ")\n",
    "\n",
    "predictions = predict(model, test_loader)\n",
    "\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'target': predictions})\n",
    "\n",
    "submission.to_csv('submissions/submission.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d5d2a",
   "metadata": {},
   "source": [
    "## LSTM Model with Multi-Input Features\n",
    "\n",
    "- use **keyword** and **location**\n",
    "- model **LSTM** to capture word order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedTweetsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, keyword_vocab=None, location_vocab=None, text_vocab=None, is_test=False):\n",
    "        self.is_test = is_test\n",
    "        if not is_test:\n",
    "            self.labels = df['target'].tolist()\n",
    "        \n",
    "        self.text = df['text'].tolist()\n",
    "        self.keywords = df['keyword'].fillna('<missing>').tolist()\n",
    "        self.locations = df['location'].fillna('<missing>').tolist()\n",
    "        \n",
    "        # token\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.tokens = [self.tokenizer(str(t).lower()) for t in self.text]\n",
    "        \n",
    "        # textvocab\n",
    "        if text_vocab is None:\n",
    "            counter = Counter()\n",
    "            for tokens in self.tokens:\n",
    "                counter.update(tokens)\n",
    "            self.text_vocab = Vocab(counter, specials=['<unk>', '<pad>'], min_freq=2)\n",
    "        else:\n",
    "            self.text_vocab = text_vocab\n",
    "        \n",
    "        # keyword vocab\n",
    "        if keyword_vocab is None:\n",
    "            unique_keywords = list(set(self.keywords))\n",
    "            self.keyword_vocab = {kw: idx for idx, kw in enumerate(unique_keywords)}\n",
    "        else:\n",
    "            self.keyword_vocab = keyword_vocab\n",
    "\n",
    "        # location vocab\n",
    "        if location_vocab is None:\n",
    "            # For locations, we'll simplify by hashing to reduce cardinality\n",
    "            self.location_vocab = {}\n",
    "            for loc in set(self.locations):\n",
    "                if loc not in self.location_vocab:\n",
    "                    self.location_vocab[loc] = len(self.location_vocab)\n",
    "        else:\n",
    "            self.location_vocab = location_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens[idx]\n",
    "        text_indices = [self.text_vocab[token] for token in tokens]\n",
    "        \n",
    "        keyword = self.keywords[idx]\n",
    "        keyword_idx = self.keyword_vocab.get(keyword, 0)  # 0 for unknown\n",
    "        \n",
    "        location = self.locations[idx]\n",
    "        location_idx = self.location_vocab.get(location, 0)  # 0 for unknown\n",
    "        \n",
    "        label = self.labels[idx] if not self.is_test else 0\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(text_indices, dtype=torch.long),\n",
    "            torch.tensor(keyword_idx, dtype=torch.long),\n",
    "            torch.tensor(location_idx, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedModel(nn.Module):\n",
    "    \"\"\"Multi-input model with LSTM for text + embeddings for keyword/location.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, keyword_size, location_size, \n",
    "                 embed_dim=128, hidden_dim=128, num_class=2, dropout=0.3):\n",
    "        super(ImprovedModel, self).__init__()\n",
    "        \n",
    "        # text embedding + LSTM\n",
    "        self.text_embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, \n",
    "                           batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        # keyword and location embeddings\n",
    "        self.keyword_embedding = nn.Embedding(keyword_size, 32)\n",
    "        self.location_embedding = nn.Embedding(location_size, 32)\n",
    "        \n",
    "        # classifier (concat LSTM output + keyword + location)\n",
    "        combined_dim = hidden_dim * 2 + 32 + 32  # bidirectional LSTM + 2 embeddings\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(combined_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, num_class)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.text_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.keyword_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.location_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, text, keyword, location):\n",
    "        # Text: [batch, seq_len] -> embed -> LSTM\n",
    "        embedded = self.text_embedding(text)  # [batch, seq, embed_dim]\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Use final hidden states from both directions\n",
    "        # hidden: [num_layers*2, batch, hidden_dim] for bidirectional\n",
    "        hidden_fwd = hidden[-2, :, :]  # forward direction, last layer\n",
    "        hidden_bwd = hidden[-1, :, :]  # backward direction, last layer\n",
    "        lstm_repr = torch.cat([hidden_fwd, hidden_bwd], dim=1)  # [batch, hidden_dim*2]\n",
    "        \n",
    "        # Keyword and location embeddings\n",
    "        keyword_emb = self.keyword_embedding(keyword)  # [batch, 32]\n",
    "        location_emb = self.location_embedding(location)  # [batch, 32]\n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined = torch.cat([lstm_repr, keyword_emb, location_emb], dim=1)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.dropout(combined)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84dde4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_improved(batch):\n",
    "    \"\"\"Collate function for padded sequences + keyword/location.\"\"\"\n",
    "    text_list, keyword_list, location_list, label_list = [], [], [], []\n",
    "    \n",
    "    for text, keyword, location, label in batch:\n",
    "        text_list.append(text)\n",
    "        keyword_list.append(keyword)\n",
    "        location_list.append(location)\n",
    "        label_list.append(label)\n",
    "    \n",
    "    # Pad text sequences to same length\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    text_padded = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    \n",
    "    keywords = torch.stack(keyword_list)\n",
    "    locations = torch.stack(location_list)\n",
    "    labels = torch.stack(label_list)\n",
    "    \n",
    "    return text_padded, keywords, locations, labels\n",
    "\n",
    "\n",
    "def train_improved(model, iterator, optimizer, criterion, scheduler=None):\n",
    "    \"\"\"Training loop for improved model.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    \n",
    "    for text, keyword, location, labels in iterator:\n",
    "        text = text.to(device)\n",
    "        keyword = keyword.to(device)\n",
    "        location = location.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(text, keyword, location)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(iterator), total_acc / total_count\n",
    "\n",
    "\n",
    "def evaluate_improved(model, iterator, criterion):\n",
    "    \"\"\"Evaluation loop for improved model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, keyword, location, labels in iterator:\n",
    "            text = text.to(device)\n",
    "            keyword = keyword.to(device)\n",
    "            location = location.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(text, keyword, location)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += (output.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(iterator), total_acc / total_count\n",
    "\n",
    "\n",
    "def predict_improved(model, iterator):\n",
    "    \"\"\"Generate predictions for improved model.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, keyword, location, _ in iterator:\n",
    "            text = text.to(device)\n",
    "            keyword = keyword.to(device)\n",
    "            location = location.to(device)\n",
    "            \n",
    "            output = model(text, keyword, location)\n",
    "            preds = output.argmax(1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create improved dataset\n",
    "improved_dataset = ImprovedTweetsDataset(train_df)\n",
    "\n",
    "print(\"Text vocab size:\", len(improved_dataset.text_vocab))\n",
    "print(\"Keyword vocab size:\", len(improved_dataset.keyword_vocab))\n",
    "print(\"Location vocab size:\", len(improved_dataset.location_vocab))\n",
    "\n",
    "# Split into train/val\n",
    "train_set_improved, val_set_improved = random_split(improved_dataset, [0.85, 0.15])\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE_IMPROVED = 32\n",
    "\n",
    "train_loader_improved = DataLoader(\n",
    "    train_set_improved,\n",
    "    batch_size=BATCH_SIZE_IMPROVED,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_improved\n",
    ")\n",
    "\n",
    "val_loader_improved = DataLoader(\n",
    "    val_set_improved,\n",
    "    batch_size=BATCH_SIZE_IMPROVED,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_improved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d371ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train improved model with better hyperparameters\n",
    "\n",
    "# Initialize model\n",
    "improved_model = ImprovedModel(\n",
    "    vocab_size=len(improved_dataset.text_vocab),\n",
    "    keyword_size=len(improved_dataset.keyword_vocab),\n",
    "    location_size=len(improved_dataset.location_vocab),\n",
    "    embed_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_class=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# Better optimizer and scheduler\n",
    "optimizer_improved = optim.Adam(improved_model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_improved, step_size=3, gamma=0.5)\n",
    "criterion_improved = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with validation monitoring\n",
    "NUM_EPOCHS = 10\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"Training improved model...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_improved(\n",
    "        improved_model, train_loader_improved, \n",
    "        optimizer_improved, criterion_improved, scheduler\n",
    "    )\n",
    "    val_loss, val_acc = evaluate_improved(\n",
    "        improved_model, val_loader_improved, criterion_improved\n",
    "    )\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(improved_model.state_dict(), 'best_model.pt')\n",
    "        print(f\"  ✓ New best model saved! (Val Acc: {val_acc:.4f})\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c927a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission with improved model\n",
    "\n",
    "# Load best model\n",
    "improved_model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Create test dataset using training vocabs\n",
    "test_dataset_improved = ImprovedTweetsDataset(\n",
    "    test_df,\n",
    "    keyword_vocab=improved_dataset.keyword_vocab,\n",
    "    location_vocab=improved_dataset.location_vocab,\n",
    "    text_vocab=improved_dataset.text_vocab,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader_improved = DataLoader(\n",
    "    test_dataset_improved,\n",
    "    batch_size=BATCH_SIZE_IMPROVED,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_improved\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "predictions_improved = predict_improved(improved_model, test_loader_improved)\n",
    "\n",
    "# Create submission\n",
    "submission_improved = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': predictions_improved\n",
    "})\n",
    "\n",
    "submission_improved.to_csv('submissions/submission_improved.csv', index=False)\n",
    "print(f\"Improved submission saved! Shape: {submission_improved.shape}\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission_improved['target'].value_counts())\n",
    "submission_improved.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf4b03",
   "metadata": {},
   "source": [
    "## Summary of Improvements\n",
    "\n",
    "### What Changed from Baseline (65% accuracy):\n",
    "\n",
    "**1. Multi-Input Features** ✓\n",
    "- Added **keyword** embeddings (32-dim)\n",
    "- Added **location** embeddings (32-dim)\n",
    "- Handles missing values with `<missing>` token\n",
    "\n",
    "**2. Better Architecture** ✓\n",
    "- **Bidirectional LSTM** (2 layers, 128 hidden units) instead of EmbeddingBag\n",
    "- Captures word order and context (critical for \"not bad\" vs \"bad\")\n",
    "- Dropout layers (0.3) to prevent overfitting\n",
    "- Combined feature representation: LSTM output + keyword + location\n",
    "\n",
    "**3. Training Improvements** ✓\n",
    "- **Adam optimizer** (lr=0.001) instead of SGD\n",
    "- **Learning rate scheduler** (decay every 3 epochs)\n",
    "- **10 epochs** with validation monitoring\n",
    "- **Gradient clipping** (prevents exploding gradients)\n",
    "- **Model checkpointing** (saves best model based on validation)\n",
    "\n",
    "**4. Data Improvements** ✓\n",
    "- Text lowercasing for consistency\n",
    "- Padded batches (variable length sequences)\n",
    "- Larger batch size (32 vs 1-16)\n",
    "\n",
    "### Expected Results:\n",
    "- Baseline: ~65% (bag-of-words, text only)\n",
    "- **Improved: 75-82%** (typical for LSTM + multi-features on this dataset)\n",
    "\n",
    "### Next Steps to Push Further (if needed):\n",
    "1. **Pre-trained embeddings**: Use GloVe or Word2Vec instead of random init\n",
    "2. **Advanced models**: BERT, RoBERTa (transformers) - can reach 85%+\n",
    "3. **Text preprocessing**: Remove URLs, handle hashtags, expand contractions\n",
    "4. **Ensemble**: Combine multiple models\n",
    "5. **Feature engineering**: Tweet length, punctuation count, capitalization patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c947b",
   "metadata": {},
   "source": [
    "## BERT Implementation (State-of-the-Art)\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained transformer model that achieves excellent results on text classification tasks.\n",
    "\n",
    "### Why BERT?\n",
    "- **Pre-trained** on massive text corpus (understands language context)\n",
    "- **Bidirectional**: Reads text in both directions simultaneously\n",
    "- **Transfer learning**: Fine-tune on your specific task with less data\n",
    "- **Expected accuracy**: 82-88% on this dataset\n",
    "\n",
    "### Requirements:\n",
    "Install transformers library: `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BERT components\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if transformers is installed\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"✓ Transformers version: {transformers.__version__}\")\n",
    "    print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"⚠ Please install transformers: pip install transformers\")\n",
    "    print(\"  Or run: !pip install transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe708d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTweetsDataset(Dataset):\n",
    "    \"\"\"Dataset for BERT with tokenization using BertTokenizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_length=128, is_test=False):\n",
    "        self.is_test = is_test\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Text data\n",
    "        self.texts = df['text'].fillna('').tolist()\n",
    "        \n",
    "        # Optional: incorporate keyword and location into text\n",
    "        # This helps BERT understand context better\n",
    "        self.keywords = df['keyword'].fillna('').tolist()\n",
    "        self.locations = df['location'].fillna('').tolist()\n",
    "        \n",
    "        # Concatenate keyword/location with text for richer context\n",
    "        self.full_texts = []\n",
    "        for i in range(len(self.texts)):\n",
    "            # Format: \"[keyword] [location] text\"\n",
    "            parts = []\n",
    "            if self.keywords[i]:\n",
    "                parts.append(f\"[{self.keywords[i]}]\")\n",
    "            if self.locations[i]:\n",
    "                parts.append(f\"[{self.locations[i]}]\")\n",
    "            parts.append(self.texts[i])\n",
    "            self.full_texts.append(' '.join(parts))\n",
    "        \n",
    "        # Labels\n",
    "        if not is_test:\n",
    "            self.labels = df['target'].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.full_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.full_texts[idx]\n",
    "        \n",
    "        # BERT tokenization\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        label = self.labels[idx] if not self.is_test else 0\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9752233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer and create datasets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create BERT datasets\n",
    "bert_dataset = BertTweetsDataset(train_df, tokenizer, max_length=128)\n",
    "\n",
    "# Split into train/val\n",
    "from torch.utils.data import random_split\n",
    "train_size = int(0.85 * len(bert_dataset))\n",
    "val_size = len(bert_dataset) - train_size\n",
    "train_bert, val_bert = random_split(bert_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train samples: {len(train_bert)}\")\n",
    "print(f\"Val samples: {len(val_bert)}\")\n",
    "print(f\"\\nSample encoding:\")\n",
    "sample = bert_dataset[0]\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a7bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    \"\"\"Train BERT for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        total_acc += (preds == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(dataloader), total_acc / total_count\n",
    "\n",
    "\n",
    "def evaluate_bert(model, dataloader, device):\n",
    "    \"\"\"Evaluate BERT model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_acc += (preds == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(dataloader), total_acc / total_count\n",
    "\n",
    "\n",
    "def predict_bert(model, dataloader, device):\n",
    "    \"\"\"Generate predictions with BERT.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afa061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training BERT model\n",
    "\n",
    "BERT_BATCH_SIZE = 16  # Smaller batch for BERT (memory intensive)\n",
    "BERT_EPOCHS = 3  # 3-4 epochs usually enough for fine-tuning\n",
    "BERT_LR = 2e-5  # Standard learning rate for BERT fine-tuning\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader_bert = DataLoader(train_bert, batch_size=BERT_BATCH_SIZE, shuffle=True)\n",
    "val_loader_bert = DataLoader(val_bert, batch_size=BERT_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize BERT model for classification\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ").to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer_bert = AdamW(bert_model.parameters(), lr=BERT_LR, eps=1e-8)\n",
    "\n",
    "total_steps = len(train_loader_bert) * BERT_EPOCHS\n",
    "scheduler_bert = get_linear_schedule_with_warmup(\n",
    "    optimizer_bert,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"BERT Model loaded: {bert_model.config.model_type}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in bert_model.parameters()):,}\")\n",
    "print(f\"Training for {BERT_EPOCHS} epochs with batch size {BERT_BATCH_SIZE}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"\\nStarting training...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "best_val_acc_bert = 0\n",
    "\n",
    "for epoch in range(BERT_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{BERT_EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc = train_bert_epoch(\n",
    "        bert_model, train_loader_bert, optimizer_bert, scheduler_bert, device\n",
    "    )\n",
    "    val_loss, val_acc = evaluate_bert(bert_model, val_loader_bert, device)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc_bert:\n",
    "        best_val_acc_bert = val_acc\n",
    "        torch.save(bert_model.state_dict(), 'best_bert_model.pt')\n",
    "        print(f\"  ✓ New best BERT model saved! (Val Acc: {val_acc:.4f})\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc_bert:.4f}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48873d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate BERT predictions for submission\n",
    "\n",
    "# Load best model\n",
    "bert_model.load_state_dict(torch.load('best_bert_model.pt'))\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset_bert = BertTweetsDataset(test_df, tokenizer, max_length=128, is_test=True)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, batch_size=BERT_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Generate predictions\n",
    "predictions_bert = predict_bert(bert_model, test_loader_bert, device)\n",
    "\n",
    "# Create submission\n",
    "submission_bert = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': predictions_bert\n",
    "})\n",
    "\n",
    "submission_bert.to_csv('submissions/submission_bert.csv', index=False)\n",
    "\n",
    "print(f\"✓ BERT submission saved to: submission_bert.csv\")\n",
    "print(f\"Shape: {submission_bert.shape}\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission_bert['target'].value_counts())\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "submission_bert.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e462b",
   "metadata": {},
   "source": [
    "## BERT Implementation Summary\n",
    "\n",
    "### What BERT does differently:\n",
    "\n",
    "**1. Pre-trained Understanding**\n",
    "- Trained on 3.3 billion words (Wikipedia + BookCorpus)\n",
    "- Already understands language context, grammar, semantics\n",
    "- We just fine-tune it for disaster classification\n",
    "\n",
    "**2. Attention Mechanism**\n",
    "- Looks at all words simultaneously\n",
    "- Understands relationships between distant words\n",
    "- Example: \"fire\" and \"building\" together → disaster\n",
    "\n",
    "**3. Contextual Embeddings**\n",
    "- Word meaning changes based on context\n",
    "- \"bank\" in \"river bank\" vs \"financial bank\"\n",
    "- BERT understands this distinction\n",
    "\n",
    "**4. Feature Integration**\n",
    "- We concatenate keyword/location with text: `[keyword] [location] text`\n",
    "- BERT processes everything together\n",
    "- No need for separate embeddings\n",
    "\n",
    "### Expected Performance:\n",
    "\n",
    "| Model | Validation Acc | Test Acc (expected) |\n",
    "|-------|----------------|---------------------|\n",
    "| Baseline (EmbeddingBag) | 65% | 65% |\n",
    "| LSTM + Features | 75-78% | 73-76% |\n",
    "| **BERT** | **82-86%** | **80-85%** |\n",
    "\n",
    "### Tips for Better BERT Results:\n",
    "\n",
    "1. **Increase epochs** (if not overfitting): 4-5 epochs\n",
    "2. **Adjust batch size**: Larger if you have GPU memory\n",
    "3. **Try different BERT variants**:\n",
    "   - `distilbert-base-uncased` - faster, 97% of BERT performance\n",
    "   - `roberta-base` - often better than BERT\n",
    "   - `bert-large-uncased` - more parameters, better accuracy\n",
    "4. **Text preprocessing**: Remove URLs, normalize hashtags\n",
    "5. **Ensemble**: Combine BERT + LSTM predictions\n",
    "\n",
    "### Memory & Speed:\n",
    "- BERT is memory-intensive (110M parameters)\n",
    "- CPU: ~5-10 min/epoch\n",
    "- GPU (recommended): ~30-60 sec/epoch\n",
    "- Reduce batch size if you get OOM errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
