{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36a5d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4409583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "816db4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                    Just happened a terrible car crash  \n",
       "1     Heard about #earthquake is different cities, s...  \n",
       "2     there is a forest fire at spot pond, geese are...  \n",
       "3              Apocalypse lighting. #Spokane #wildfires  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "3259  Storm in RI worse than last hurricane. My city...  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1107e2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword - missing: 61 (0.8%)\n",
      "Keyword - unique: 221\n",
      "\n",
      "Location - missing: 2533 (33.3%)\n",
      "Location - unique: 3341\n",
      "\n",
      "Top keywords:\n",
      "keyword\n",
      "fatalities     45\n",
      "deluge         42\n",
      "armageddon     42\n",
      "damage         41\n",
      "body%20bags    41\n",
      "harm           41\n",
      "sinking        41\n",
      "evacuate       40\n",
      "outbreak       40\n",
      "fear           40\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Explore keyword and location features\n",
    "print(\"Keyword - missing:\", train_df['keyword'].isna().sum(), f\"({train_df['keyword'].isna().mean()*100:.1f}%)\")\n",
    "print(\"Keyword - unique:\", train_df['keyword'].nunique())\n",
    "print(\"\\nLocation - missing:\", train_df['location'].isna().sum(), f\"({train_df['location'].isna().mean()*100:.1f}%)\")\n",
    "print(\"Location - unique:\", train_df['location'].nunique())\n",
    "print(\"\\nTop keywords:\")\n",
    "print(train_df['keyword'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c051911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['target'].tolist()\n",
    "        self.text = df['text'].tolist()\n",
    "\n",
    "        # Tokenizer\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.tokens = [self.tokenizer(t) for t in self.text]\n",
    "\n",
    "        # Build vocab\n",
    "        counter = Counter()\n",
    "        for tokens in self.tokens:\n",
    "            counter.update(tokens)\n",
    "        self.vocab = Vocab(counter, specials=['<unk>', '<pad>'], min_freq=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens[idx]\n",
    "        indices = [self.vocab[token] for token in tokens]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def print_vocab(self):\n",
    "        print(\"Vocab size:\", len(self.vocab))\n",
    "        print(\"Top tokens:\", self.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3f3def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6472, 1141)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TweetsDataset(train_df)\n",
    "len(dataset)\n",
    "train_set, val_set = random_split(dataset, [.85, .15])\n",
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, hidden_size=64):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.hidden = nn.Linear(embed_dim, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = self.hidden(embedded)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47278b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_bag(batch: list[tuple[torch.Tensor, torch.Tensor]]):\n",
    "\n",
    "    # batch: list of (text_tensor_1d, label)\n",
    "    text_list, label_list = [], []\n",
    "    for text, label in batch:\n",
    "        text_list.append(text)\n",
    "        label_list.append(label)\n",
    "\n",
    "    # Build offsets for EmbeddingBag\n",
    "    lengths = torch.tensor([t.size(0) for t in text_list], dtype=torch.long)\n",
    "    offsets = torch.cat((torch.tensor([0], dtype=torch.long), torch.cumsum(lengths, dim=0)[:-1]))\n",
    "    text = torch.cat(text_list, dim=0)\n",
    "    labels = torch.stack(label_list).long()\n",
    "\n",
    "    # print(text, offsets, labels)\n",
    "    return text, offsets, labels\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 100\n",
    "\n",
    "    for idx, (text, offsets, labels) in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, labels = text.to(device), offsets.to(device), labels.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(f'At batch {idx}, accuracy: {total_acc/total_count:.4f}')\n",
    "            total_acc, total_count = 0, 0\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text, offsets, labels in iterator:\n",
    "            text, offsets, labels = text.to(device), offsets.to(device), labels.to(device)\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, labels)\n",
    "            total_acc += (output.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "    return total_acc / total_count\n",
    "\n",
    "\n",
    "def predict(model, iterator):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for text, offsets, labels in iterator:\n",
    "            text, offsets = text.to(device), offsets.to(device)\n",
    "            output = model(text, offsets)\n",
    "            preds = output.argmax(1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2e8b4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At batch 100, accuracy: 0.4851\n",
      "At batch 200, accuracy: 0.5500\n",
      "At batch 300, accuracy: 0.4800\n",
      "At batch 400, accuracy: 0.4900\n",
      "At batch 500, accuracy: 0.5100\n",
      "At batch 600, accuracy: 0.5600\n",
      "At batch 700, accuracy: 0.6300\n",
      "At batch 800, accuracy: 0.5300\n",
      "At batch 900, accuracy: 0.5000\n",
      "At batch 1000, accuracy: 0.5100\n",
      "At batch 1100, accuracy: 0.6000\n",
      "At batch 1200, accuracy: 0.4300\n",
      "At batch 1300, accuracy: 0.7200\n",
      "At batch 1400, accuracy: 0.5900\n",
      "At batch 1500, accuracy: 0.6300\n",
      "At batch 1600, accuracy: 0.6300\n",
      "At batch 1700, accuracy: 0.5900\n",
      "At batch 1800, accuracy: 0.5900\n",
      "At batch 1900, accuracy: 0.5800\n",
      "At batch 2000, accuracy: 0.5100\n",
      "At batch 2100, accuracy: 0.6100\n",
      "At batch 2200, accuracy: 0.6400\n",
      "At batch 2300, accuracy: 0.6300\n",
      "At batch 2400, accuracy: 0.5300\n",
      "At batch 2500, accuracy: 0.5500\n",
      "At batch 2600, accuracy: 0.5900\n",
      "At batch 2700, accuracy: 0.5400\n",
      "At batch 2800, accuracy: 0.6400\n",
      "At batch 2900, accuracy: 0.5900\n",
      "At batch 3000, accuracy: 0.5700\n",
      "At batch 3100, accuracy: 0.6100\n",
      "At batch 3200, accuracy: 0.6400\n",
      "At batch 3300, accuracy: 0.5800\n",
      "At batch 3400, accuracy: 0.6000\n",
      "At batch 3500, accuracy: 0.6200\n",
      "At batch 3600, accuracy: 0.6200\n",
      "At batch 3700, accuracy: 0.6200\n",
      "At batch 3800, accuracy: 0.5900\n",
      "At batch 3900, accuracy: 0.5400\n",
      "At batch 4000, accuracy: 0.5600\n",
      "At batch 4100, accuracy: 0.5500\n",
      "At batch 4200, accuracy: 0.5200\n",
      "At batch 4300, accuracy: 0.5300\n",
      "At batch 4400, accuracy: 0.6600\n",
      "At batch 4500, accuracy: 0.5800\n",
      "At batch 4600, accuracy: 0.5100\n",
      "At batch 4700, accuracy: 0.5600\n",
      "At batch 4800, accuracy: 0.5700\n",
      "At batch 4900, accuracy: 0.5800\n",
      "At batch 5000, accuracy: 0.5600\n",
      "At batch 5100, accuracy: 0.5500\n",
      "At batch 5200, accuracy: 0.5900\n",
      "At batch 5300, accuracy: 0.6100\n",
      "At batch 5400, accuracy: 0.6400\n",
      "At batch 5500, accuracy: 0.6800\n",
      "At batch 5600, accuracy: 0.5700\n",
      "At batch 5700, accuracy: 0.5900\n",
      "At batch 5800, accuracy: 0.6700\n",
      "At batch 5900, accuracy: 0.6400\n",
      "At batch 6000, accuracy: 0.6000\n",
      "At batch 6100, accuracy: 0.6300\n",
      "At batch 6200, accuracy: 0.6500\n",
      "At batch 6300, accuracy: 0.6100\n",
      "At batch 6400, accuracy: 0.5700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6389132340052586"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "iterator = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch_bag\n",
    ")\n",
    "\n",
    "model = TextClassificationModel(vocab_size=len(dataset.vocab), embed_dim=64, num_class=2).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train(model, iterator, optimizer, criterion)\n",
    "\n",
    "evaluate_iterator = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch_bag\n",
    ")\n",
    "\n",
    "evaluate(model, evaluate_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f23298f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved! Shape: (3263, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       0\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create submission for test set\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, vocab, tokenizer):\n",
    "        self.text = df['text'].tolist()\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokens = [self.tokenizer(t) for t in self.text]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens[idx]\n",
    "        indices = [self.vocab[token] for token in tokens]\n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "# Create test dataset using the vocab from training\n",
    "test_dataset = TestDataset(test_df, dataset.vocab, dataset.tokenizer)\n",
    "\n",
    "# Create dataloader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch_bag\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = predict(model, test_loader)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Submission saved! Shape: {submission.shape}\")\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d5d2a",
   "metadata": {},
   "source": [
    "## Improved Model with Multi-Input Features\n",
    "\n",
    "This enhanced version:\n",
    "- Uses **keyword** and **location** as additional categorical features via embeddings\n",
    "- Implements **LSTM** to capture word order (not just bag-of-words)\n",
    "- Better handles missing values\n",
    "- Improved training with Adam optimizer and learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f276585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedTweetsDataset(Dataset):\n",
    "    \"\"\"Dataset with keyword and location features + better text preprocessing.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, keyword_vocab=None, location_vocab=None, text_vocab=None, is_test=False):\n",
    "        self.is_test = is_test\n",
    "        if not is_test:\n",
    "            self.labels = df['target'].tolist()\n",
    "        \n",
    "        self.text = df['text'].tolist()\n",
    "        self.keywords = df['keyword'].fillna('<missing>').tolist()\n",
    "        self.locations = df['location'].fillna('<missing>').tolist()\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.tokens = [self.tokenizer(str(t).lower()) for t in self.text]\n",
    "        \n",
    "        # Build or reuse text vocab\n",
    "        if text_vocab is None:\n",
    "            counter = Counter()\n",
    "            for tokens in self.tokens:\n",
    "                counter.update(tokens)\n",
    "            self.text_vocab = Vocab(counter, specials=['<unk>', '<pad>'], min_freq=2)\n",
    "        else:\n",
    "            self.text_vocab = text_vocab\n",
    "        \n",
    "        # Build or reuse keyword vocab\n",
    "        if keyword_vocab is None:\n",
    "            unique_keywords = list(set(self.keywords))\n",
    "            self.keyword_vocab = {kw: idx for idx, kw in enumerate(unique_keywords)}\n",
    "        else:\n",
    "            self.keyword_vocab = keyword_vocab\n",
    "        \n",
    "        # Build or reuse location vocab (simplified - just use keyword approach)\n",
    "        if location_vocab is None:\n",
    "            # For locations, we'll simplify by hashing to reduce cardinality\n",
    "            self.location_vocab = {}\n",
    "            for loc in set(self.locations):\n",
    "                if loc not in self.location_vocab:\n",
    "                    self.location_vocab[loc] = len(self.location_vocab)\n",
    "        else:\n",
    "            self.location_vocab = location_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Text tokens\n",
    "        tokens = self.tokens[idx]\n",
    "        text_indices = [self.text_vocab[token] for token in tokens]\n",
    "        \n",
    "        # Keyword index\n",
    "        keyword = self.keywords[idx]\n",
    "        keyword_idx = self.keyword_vocab.get(keyword, 0)  # 0 for unknown\n",
    "        \n",
    "        # Location index\n",
    "        location = self.locations[idx]\n",
    "        location_idx = self.location_vocab.get(location, 0)  # 0 for unknown\n",
    "        \n",
    "        # Label\n",
    "        label = self.labels[idx] if not self.is_test else 0\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(text_indices, dtype=torch.long),\n",
    "            torch.tensor(keyword_idx, dtype=torch.long),\n",
    "            torch.tensor(location_idx, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3eea1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedModel(nn.Module):\n",
    "    \"\"\"Multi-input model with LSTM for text + embeddings for keyword/location.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, keyword_size, location_size, \n",
    "                 embed_dim=128, hidden_dim=128, num_class=2, dropout=0.3):\n",
    "        super(ImprovedModel, self).__init__()\n",
    "        \n",
    "        # Text embedding + LSTM\n",
    "        self.text_embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, \n",
    "                           batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        # Keyword and location embeddings\n",
    "        self.keyword_embedding = nn.Embedding(keyword_size, 32)\n",
    "        self.location_embedding = nn.Embedding(location_size, 32)\n",
    "        \n",
    "        # Classifier (concat LSTM output + keyword + location)\n",
    "        combined_dim = hidden_dim * 2 + 32 + 32  # bidirectional LSTM + 2 embeddings\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(combined_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, num_class)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.text_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.keyword_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.location_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, text, keyword, location):\n",
    "        # Text: [batch, seq_len] -> embed -> LSTM\n",
    "        embedded = self.text_embedding(text)  # [batch, seq, embed_dim]\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Use final hidden states from both directions\n",
    "        # hidden: [num_layers*2, batch, hidden_dim] for bidirectional\n",
    "        hidden_fwd = hidden[-2, :, :]  # forward direction, last layer\n",
    "        hidden_bwd = hidden[-1, :, :]  # backward direction, last layer\n",
    "        lstm_repr = torch.cat([hidden_fwd, hidden_bwd], dim=1)  # [batch, hidden_dim*2]\n",
    "        \n",
    "        # Keyword and location embeddings\n",
    "        keyword_emb = self.keyword_embedding(keyword)  # [batch, 32]\n",
    "        location_emb = self.location_embedding(location)  # [batch, 32]\n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined = torch.cat([lstm_repr, keyword_emb, location_emb], dim=1)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.dropout(combined)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a84dde4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_improved(batch):\n",
    "    \"\"\"Collate function for padded sequences + keyword/location.\"\"\"\n",
    "    text_list, keyword_list, location_list, label_list = [], [], [], []\n",
    "    \n",
    "    for text, keyword, location, label in batch:\n",
    "        text_list.append(text)\n",
    "        keyword_list.append(keyword)\n",
    "        location_list.append(location)\n",
    "        label_list.append(label)\n",
    "    \n",
    "    # Pad text sequences to same length\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    text_padded = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    \n",
    "    keywords = torch.stack(keyword_list)\n",
    "    locations = torch.stack(location_list)\n",
    "    labels = torch.stack(label_list)\n",
    "    \n",
    "    return text_padded, keywords, locations, labels\n",
    "\n",
    "\n",
    "def train_improved(model, iterator, optimizer, criterion, scheduler=None):\n",
    "    \"\"\"Training loop for improved model.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    \n",
    "    for text, keyword, location, labels in iterator:\n",
    "        text = text.to(device)\n",
    "        keyword = keyword.to(device)\n",
    "        location = location.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(text, keyword, location)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(iterator), total_acc / total_count\n",
    "\n",
    "\n",
    "def evaluate_improved(model, iterator, criterion):\n",
    "    \"\"\"Evaluation loop for improved model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, keyword, location, labels in iterator:\n",
    "            text = text.to(device)\n",
    "            keyword = keyword.to(device)\n",
    "            location = location.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(text, keyword, location)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += (output.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(iterator), total_acc / total_count\n",
    "\n",
    "\n",
    "def predict_improved(model, iterator):\n",
    "    \"\"\"Generate predictions for improved model.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, keyword, location, _ in iterator:\n",
    "            text = text.to(device)\n",
    "            keyword = keyword.to(device)\n",
    "            location = location.to(device)\n",
    "            \n",
    "            output = model(text, keyword, location)\n",
    "            preds = output.argmax(1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e10cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vocab size: 6982\n",
      "Keyword vocab size: 222\n",
      "Location vocab size: 3342\n"
     ]
    }
   ],
   "source": [
    "# Create improved dataset\n",
    "improved_dataset = ImprovedTweetsDataset(train_df)\n",
    "\n",
    "print(\"Text vocab size:\", len(improved_dataset.text_vocab))\n",
    "print(\"Keyword vocab size:\", len(improved_dataset.keyword_vocab))\n",
    "print(\"Location vocab size:\", len(improved_dataset.location_vocab))\n",
    "\n",
    "# Split into train/val\n",
    "train_set_improved, val_set_improved = random_split(improved_dataset, [0.85, 0.15])\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE_IMPROVED = 32\n",
    "\n",
    "train_loader_improved = DataLoader(\n",
    "    train_set_improved,\n",
    "    batch_size=BATCH_SIZE_IMPROVED,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_improved\n",
    ")\n",
    "\n",
    "val_loader_improved = DataLoader(\n",
    "    val_set_improved,\n",
    "    batch_size=BATCH_SIZE_IMPROVED,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_improved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30d371ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training improved model...\n",
      "Epoch 1/10\n",
      "  Train Loss: 0.5575, Train Acc: 0.7086\n",
      "  Val Loss: 0.4389, Val Acc: 0.8028\n",
      "  ✓ New best model saved! (Val Acc: 0.8028)\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.3668, Train Acc: 0.8514\n",
      "  Val Loss: 0.4568, Val Acc: 0.8019\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.2765, Train Acc: 0.8971\n",
      "  Val Loss: 0.4621, Val Acc: 0.7809\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.1890, Train Acc: 0.9356\n",
      "  Val Loss: 0.6138, Val Acc: 0.7862\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.1378, Train Acc: 0.9527\n",
      "  Val Loss: 0.6761, Val Acc: 0.7748\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.1047, Train Acc: 0.9631\n",
      "  Val Loss: 0.8079, Val Acc: 0.7713\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.0677, Train Acc: 0.9742\n",
      "  Val Loss: 0.9675, Val Acc: 0.7713\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.0575, Train Acc: 0.9793\n",
      "  Val Loss: 1.0492, Val Acc: 0.7695\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.0488, Train Acc: 0.9816\n",
      "  Val Loss: 1.2112, Val Acc: 0.7669\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.0397, Train Acc: 0.9842\n",
      "  Val Loss: 1.3019, Val Acc: 0.7651\n",
      "\n",
      "Best validation accuracy: 0.8028\n"
     ]
    }
   ],
   "source": [
    "# Train improved model with better hyperparameters\n",
    "\n",
    "# Initialize model\n",
    "improved_model = ImprovedModel(\n",
    "    vocab_size=len(improved_dataset.text_vocab),\n",
    "    keyword_size=len(improved_dataset.keyword_vocab),\n",
    "    location_size=len(improved_dataset.location_vocab),\n",
    "    embed_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_class=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# Better optimizer and scheduler\n",
    "optimizer_improved = optim.Adam(improved_model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_improved, step_size=3, gamma=0.5)\n",
    "criterion_improved = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with validation monitoring\n",
    "NUM_EPOCHS = 10\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"Training improved model...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_improved(\n",
    "        improved_model, train_loader_improved, \n",
    "        optimizer_improved, criterion_improved, scheduler\n",
    "    )\n",
    "    val_loss, val_acc = evaluate_improved(\n",
    "        improved_model, val_loader_improved, criterion_improved\n",
    "    )\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(improved_model.state_dict(), 'best_model.pt')\n",
    "        print(f\"  ✓ New best model saved! (Val Acc: {val_acc:.4f})\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c927a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved submission saved! Shape: (3263, 2)\n",
      "\n",
      "Prediction distribution:\n",
      "target\n",
      "0    2201\n",
      "1    1062\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate submission with improved model\n",
    "\n",
    "# Load best model\n",
    "improved_model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Create test dataset using training vocabs\n",
    "test_dataset_improved = ImprovedTweetsDataset(\n",
    "    test_df,\n",
    "    keyword_vocab=improved_dataset.keyword_vocab,\n",
    "    location_vocab=improved_dataset.location_vocab,\n",
    "    text_vocab=improved_dataset.text_vocab,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader_improved = DataLoader(\n",
    "    test_dataset_improved,\n",
    "    batch_size=BATCH_SIZE_IMPROVED,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_improved\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "predictions_improved = predict_improved(improved_model, test_loader_improved)\n",
    "\n",
    "# Create submission\n",
    "submission_improved = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': predictions_improved\n",
    "})\n",
    "\n",
    "submission_improved.to_csv('submission_improved.csv', index=False)\n",
    "print(f\"Improved submission saved! Shape: {submission_improved.shape}\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission_improved['target'].value_counts())\n",
    "submission_improved.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf4b03",
   "metadata": {},
   "source": [
    "## Summary of Improvements\n",
    "\n",
    "### What Changed from Baseline (65% accuracy):\n",
    "\n",
    "**1. Multi-Input Features** ✓\n",
    "- Added **keyword** embeddings (32-dim)\n",
    "- Added **location** embeddings (32-dim)\n",
    "- Handles missing values with `<missing>` token\n",
    "\n",
    "**2. Better Architecture** ✓\n",
    "- **Bidirectional LSTM** (2 layers, 128 hidden units) instead of EmbeddingBag\n",
    "- Captures word order and context (critical for \"not bad\" vs \"bad\")\n",
    "- Dropout layers (0.3) to prevent overfitting\n",
    "- Combined feature representation: LSTM output + keyword + location\n",
    "\n",
    "**3. Training Improvements** ✓\n",
    "- **Adam optimizer** (lr=0.001) instead of SGD\n",
    "- **Learning rate scheduler** (decay every 3 epochs)\n",
    "- **10 epochs** with validation monitoring\n",
    "- **Gradient clipping** (prevents exploding gradients)\n",
    "- **Model checkpointing** (saves best model based on validation)\n",
    "\n",
    "**4. Data Improvements** ✓\n",
    "- Text lowercasing for consistency\n",
    "- Padded batches (variable length sequences)\n",
    "- Larger batch size (32 vs 1-16)\n",
    "\n",
    "### Expected Results:\n",
    "- Baseline: ~65% (bag-of-words, text only)\n",
    "- **Improved: 75-82%** (typical for LSTM + multi-features on this dataset)\n",
    "\n",
    "### Next Steps to Push Further (if needed):\n",
    "1. **Pre-trained embeddings**: Use GloVe or Word2Vec instead of random init\n",
    "2. **Advanced models**: BERT, RoBERTa (transformers) - can reach 85%+\n",
    "3. **Text preprocessing**: Remove URLs, handle hashtags, expand contractions\n",
    "4. **Ensemble**: Combine multiple models\n",
    "5. **Feature engineering**: Tweet length, punctuation count, capitalization patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
