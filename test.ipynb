{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36a5d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4409583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "816db4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                    Just happened a terrible car crash  \n",
       "1     Heard about #earthquake is different cities, s...  \n",
       "2     there is a forest fire at spot pond, geese are...  \n",
       "3              Apocalypse lighting. #Spokane #wildfires  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "3259  Storm in RI worse than last hurricane. My city...  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1107e2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword - missing: 61 (0.8%)\n",
      "Keyword - unique: 221\n",
      "\n",
      "Location - missing: 2533 (33.3%)\n",
      "Location - unique: 3341\n",
      "\n",
      "Top keywords:\n",
      "keyword\n",
      "fatalities     45\n",
      "deluge         42\n",
      "armageddon     42\n",
      "damage         41\n",
      "body%20bags    41\n",
      "harm           41\n",
      "sinking        41\n",
      "evacuate       40\n",
      "outbreak       40\n",
      "fear           40\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Explore keyword and location features\n",
    "print(\"Keyword - missing:\", train_df['keyword'].isna().sum(), f\"({train_df['keyword'].isna().mean()*100:.1f}%)\")\n",
    "print(\"Keyword - unique:\", train_df['keyword'].nunique())\n",
    "print(\"\\nLocation - missing:\", train_df['location'].isna().sum(), f\"({train_df['location'].isna().mean()*100:.1f}%)\")\n",
    "print(\"Location - unique:\", train_df['location'].nunique())\n",
    "print(\"\\nTop keywords:\")\n",
    "print(train_df['keyword'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c051911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['target'].tolist()\n",
    "        self.text = df['text'].tolist()\n",
    "\n",
    "        # Tokenizer\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.tokens = [self.tokenizer(t) for t in self.text]\n",
    "\n",
    "        # Build vocab\n",
    "        counter = Counter()\n",
    "        for tokens in self.tokens:\n",
    "            counter.update(tokens)\n",
    "        self.vocab = Vocab(counter, specials=['<unk>', '<pad>'], min_freq=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens[idx]\n",
    "        indices = [self.vocab[token] for token in tokens]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def print_vocab(self):\n",
    "        print(\"Vocab size:\", len(self.vocab))\n",
    "        print(\"Top tokens:\", self.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3f3def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6472, 1141)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TweetsDataset(train_df)\n",
    "len(dataset)\n",
    "train_set, val_set = random_split(dataset, [.85, .15])\n",
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, hidden_size=64):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.hidden = nn.Linear(embed_dim, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = self.hidden(embedded)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47278b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_bag(batch: list[tuple[torch.Tensor, torch.Tensor]]):\n",
    "\n",
    "    # batch: list of (text_tensor_1d, label)\n",
    "    text_list, label_list = [], []\n",
    "    for text, label in batch:\n",
    "        text_list.append(text)\n",
    "        label_list.append(label)\n",
    "\n",
    "    # Build offsets for EmbeddingBag\n",
    "    lengths = torch.tensor([t.size(0) for t in text_list], dtype=torch.long)\n",
    "    offsets = torch.cat((torch.tensor([0], dtype=torch.long), torch.cumsum(lengths, dim=0)[:-1]))\n",
    "    text = torch.cat(text_list, dim=0)\n",
    "    labels = torch.stack(label_list).long()\n",
    "\n",
    "    # print(text, offsets, labels)\n",
    "    return text, offsets, labels\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 100\n",
    "\n",
    "    for idx, (text, offsets, labels) in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, labels = text.to(device), offsets.to(device), labels.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(f'At batch {idx}, accuracy: {total_acc/total_count:.4f}')\n",
    "            total_acc, total_count = 0, 0\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text, offsets, labels in iterator:\n",
    "            text, offsets, labels = text.to(device), offsets.to(device), labels.to(device)\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, labels)\n",
    "            total_acc += (output.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "    return total_acc / total_count\n",
    "\n",
    "\n",
    "def predict(model, iterator):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for text, offsets, labels in iterator:\n",
    "            text, offsets = text.to(device), offsets.to(device)\n",
    "            output = model(text, offsets)\n",
    "            preds = output.argmax(1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2e8b4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At batch 100, accuracy: 0.4851\n",
      "At batch 200, accuracy: 0.5500\n",
      "At batch 300, accuracy: 0.4800\n",
      "At batch 400, accuracy: 0.4900\n",
      "At batch 500, accuracy: 0.5100\n",
      "At batch 600, accuracy: 0.5600\n",
      "At batch 700, accuracy: 0.6300\n",
      "At batch 800, accuracy: 0.5300\n",
      "At batch 900, accuracy: 0.5000\n",
      "At batch 1000, accuracy: 0.5100\n",
      "At batch 1100, accuracy: 0.6000\n",
      "At batch 1200, accuracy: 0.4300\n",
      "At batch 1300, accuracy: 0.7200\n",
      "At batch 1400, accuracy: 0.5900\n",
      "At batch 1500, accuracy: 0.6300\n",
      "At batch 1600, accuracy: 0.6300\n",
      "At batch 1700, accuracy: 0.5900\n",
      "At batch 1800, accuracy: 0.5900\n",
      "At batch 1900, accuracy: 0.5800\n",
      "At batch 2000, accuracy: 0.5100\n",
      "At batch 2100, accuracy: 0.6100\n",
      "At batch 2200, accuracy: 0.6400\n",
      "At batch 2300, accuracy: 0.6300\n",
      "At batch 2400, accuracy: 0.5300\n",
      "At batch 2500, accuracy: 0.5500\n",
      "At batch 2600, accuracy: 0.5900\n",
      "At batch 2700, accuracy: 0.5400\n",
      "At batch 2800, accuracy: 0.6400\n",
      "At batch 2900, accuracy: 0.5900\n",
      "At batch 3000, accuracy: 0.5700\n",
      "At batch 3100, accuracy: 0.6100\n",
      "At batch 3200, accuracy: 0.6400\n",
      "At batch 3300, accuracy: 0.5800\n",
      "At batch 3400, accuracy: 0.6000\n",
      "At batch 3500, accuracy: 0.6200\n",
      "At batch 3600, accuracy: 0.6200\n",
      "At batch 3700, accuracy: 0.6200\n",
      "At batch 3800, accuracy: 0.5900\n",
      "At batch 3900, accuracy: 0.5400\n",
      "At batch 4000, accuracy: 0.5600\n",
      "At batch 4100, accuracy: 0.5500\n",
      "At batch 4200, accuracy: 0.5200\n",
      "At batch 4300, accuracy: 0.5300\n",
      "At batch 4400, accuracy: 0.6600\n",
      "At batch 4500, accuracy: 0.5800\n",
      "At batch 4600, accuracy: 0.5100\n",
      "At batch 4700, accuracy: 0.5600\n",
      "At batch 4800, accuracy: 0.5700\n",
      "At batch 4900, accuracy: 0.5800\n",
      "At batch 5000, accuracy: 0.5600\n",
      "At batch 5100, accuracy: 0.5500\n",
      "At batch 5200, accuracy: 0.5900\n",
      "At batch 5300, accuracy: 0.6100\n",
      "At batch 5400, accuracy: 0.6400\n",
      "At batch 5500, accuracy: 0.6800\n",
      "At batch 5600, accuracy: 0.5700\n",
      "At batch 5700, accuracy: 0.5900\n",
      "At batch 5800, accuracy: 0.6700\n",
      "At batch 5900, accuracy: 0.6400\n",
      "At batch 6000, accuracy: 0.6000\n",
      "At batch 6100, accuracy: 0.6300\n",
      "At batch 6200, accuracy: 0.6500\n",
      "At batch 6300, accuracy: 0.6100\n",
      "At batch 6400, accuracy: 0.5700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6389132340052586"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "iterator = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch_bag\n",
    ")\n",
    "\n",
    "model = TextClassificationModel(vocab_size=len(dataset.vocab), embed_dim=64, num_class=2).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train(model, iterator, optimizer, criterion)\n",
    "\n",
    "evaluate_iterator = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch_bag\n",
    ")\n",
    "\n",
    "evaluate(model, evaluate_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f23298f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved! Shape: (3263, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       0\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create submission for test set\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, vocab, tokenizer):\n",
    "        self.text = df['text'].tolist()\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokens = [self.tokenizer(t) for t in self.text]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens[idx]\n",
    "        indices = [self.vocab[token] for token in tokens]\n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "# Create test dataset using the vocab from training\n",
    "test_dataset = TestDataset(test_df, dataset.vocab, dataset.tokenizer)\n",
    "\n",
    "# Create dataloader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch_bag\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = predict(model, test_loader)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Submission saved! Shape: {submission.shape}\")\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d5d2a",
   "metadata": {},
   "source": [
    "## Improved Model with Multi-Input Features\n",
    "\n",
    "This enhanced version:\n",
    "- Uses **keyword** and **location** as additional categorical features via embeddings\n",
    "- Implements **LSTM** to capture word order (not just bag-of-words)\n",
    "- Better handles missing values\n",
    "- Improved training with Adam optimizer and learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f276585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedTweetsDataset(Dataset):\n",
    "    \"\"\"Dataset with keyword and location features + better text preprocessing.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, keyword_vocab=None, location_vocab=None, text_vocab=None, is_test=False):\n",
    "        self.is_test = is_test\n",
    "        if not is_test:\n",
    "            self.labels = df['target'].tolist()\n",
    "        \n",
    "        self.text = df['text'].tolist()\n",
    "        self.keywords = df['keyword'].fillna('<missing>').tolist()\n",
    "        self.locations = df['location'].fillna('<missing>').tolist()\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.tokens = [self.tokenizer(str(t).lower()) for t in self.text]\n",
    "        \n",
    "        # Build or reuse text vocab\n",
    "        if text_vocab is None:\n",
    "            counter = Counter()\n",
    "            for tokens in self.tokens:\n",
    "                counter.update(tokens)\n",
    "            self.text_vocab = Vocab(counter, specials=['<unk>', '<pad>'], min_freq=2)\n",
    "        else:\n",
    "            self.text_vocab = text_vocab\n",
    "        \n",
    "        # Build or reuse keyword vocab\n",
    "        if keyword_vocab is None:\n",
    "            unique_keywords = list(set(self.keywords))\n",
    "            self.keyword_vocab = {kw: idx for idx, kw in enumerate(unique_keywords)}\n",
    "        else:\n",
    "            self.keyword_vocab = keyword_vocab\n",
    "        \n",
    "        # Build or reuse location vocab (simplified - just use keyword approach)\n",
    "        if location_vocab is None:\n",
    "            # For locations, we'll simplify by hashing to reduce cardinality\n",
    "            self.location_vocab = {}\n",
    "            for loc in set(self.locations):\n",
    "                if loc not in self.location_vocab:\n",
    "                    self.location_vocab[loc] = len(self.location_vocab)\n",
    "        else:\n",
    "            self.location_vocab = location_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Text tokens\n",
    "        tokens = self.tokens[idx]\n",
    "        text_indices = [self.text_vocab[token] for token in tokens]\n",
    "        \n",
    "        # Keyword index\n",
    "        keyword = self.keywords[idx]\n",
    "        keyword_idx = self.keyword_vocab.get(keyword, 0)  # 0 for unknown\n",
    "        \n",
    "        # Location index\n",
    "        location = self.locations[idx]\n",
    "        location_idx = self.location_vocab.get(location, 0)  # 0 for unknown\n",
    "        \n",
    "        # Label\n",
    "        label = self.labels[idx] if not self.is_test else 0\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(text_indices, dtype=torch.long),\n",
    "            torch.tensor(keyword_idx, dtype=torch.long),\n",
    "            torch.tensor(location_idx, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3eea1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedModel(nn.Module):\n",
    "    \"\"\"Multi-input model with LSTM for text + embeddings for keyword/location.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, keyword_size, location_size, \n",
    "                 embed_dim=128, hidden_dim=128, num_class=2, dropout=0.3):\n",
    "        super(ImprovedModel, self).__init__()\n",
    "        \n",
    "        # Text embedding + LSTM\n",
    "        self.text_embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, \n",
    "                           batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        # Keyword and location embeddings\n",
    "        self.keyword_embedding = nn.Embedding(keyword_size, 32)\n",
    "        self.location_embedding = nn.Embedding(location_size, 32)\n",
    "        \n",
    "        # Classifier (concat LSTM output + keyword + location)\n",
    "        combined_dim = hidden_dim * 2 + 32 + 32  # bidirectional LSTM + 2 embeddings\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(combined_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, num_class)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.text_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.keyword_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.location_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, text, keyword, location):\n",
    "        # Text: [batch, seq_len] -> embed -> LSTM\n",
    "        embedded = self.text_embedding(text)  # [batch, seq, embed_dim]\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Use final hidden states from both directions\n",
    "        # hidden: [num_layers*2, batch, hidden_dim] for bidirectional\n",
    "        hidden_fwd = hidden[-2, :, :]  # forward direction, last layer\n",
    "        hidden_bwd = hidden[-1, :, :]  # backward direction, last layer\n",
    "        lstm_repr = torch.cat([hidden_fwd, hidden_bwd], dim=1)  # [batch, hidden_dim*2]\n",
    "        \n",
    "        # Keyword and location embeddings\n",
    "        keyword_emb = self.keyword_embedding(keyword)  # [batch, 32]\n",
    "        location_emb = self.location_embedding(location)  # [batch, 32]\n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined = torch.cat([lstm_repr, keyword_emb, location_emb], dim=1)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.dropout(combined)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a84dde4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_improved(batch):\n",
    "    \"\"\"Collate function for padded sequences + keyword/location.\"\"\"\n",
    "    text_list, keyword_list, location_list, label_list = [], [], [], []\n",
    "    \n",
    "    for text, keyword, location, label in batch:\n",
    "        text_list.append(text)\n",
    "        keyword_list.append(keyword)\n",
    "        location_list.append(location)\n",
    "        label_list.append(label)\n",
    "    \n",
    "    # Pad text sequences to same length\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    text_padded = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    \n",
    "    keywords = torch.stack(keyword_list)\n",
    "    locations = torch.stack(location_list)\n",
    "    labels = torch.stack(label_list)\n",
    "    \n",
    "    return text_padded, keywords, locations, labels\n",
    "\n",
    "\n",
    "def train_improved(model, iterator, optimizer, criterion, scheduler=None):\n",
    "    \"\"\"Training loop for improved model.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    \n",
    "    for text, keyword, location, labels in iterator:\n",
    "        text = text.to(device)\n",
    "        keyword = keyword.to(device)\n",
    "        location = location.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(text, keyword, location)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(iterator), total_acc / total_count\n",
    "\n",
    "\n",
    "def evaluate_improved(model, iterator, criterion):\n",
    "    \"\"\"Evaluation loop for improved model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, keyword, location, labels in iterator:\n",
    "            text = text.to(device)\n",
    "            keyword = keyword.to(device)\n",
    "            location = location.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(text, keyword, location)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += (output.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(iterator), total_acc / total_count\n",
    "\n",
    "\n",
    "def predict_improved(model, iterator):\n",
    "    \"\"\"Generate predictions for improved model.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, keyword, location, _ in iterator:\n",
    "            text = text.to(device)\n",
    "            keyword = keyword.to(device)\n",
    "            location = location.to(device)\n",
    "            \n",
    "            output = model(text, keyword, location)\n",
    "            preds = output.argmax(1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e10cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vocab size: 6982\n",
      "Keyword vocab size: 222\n",
      "Location vocab size: 3342\n"
     ]
    }
   ],
   "source": [
    "# Create improved dataset\n",
    "improved_dataset = ImprovedTweetsDataset(train_df)\n",
    "\n",
    "print(\"Text vocab size:\", len(improved_dataset.text_vocab))\n",
    "print(\"Keyword vocab size:\", len(improved_dataset.keyword_vocab))\n",
    "print(\"Location vocab size:\", len(improved_dataset.location_vocab))\n",
    "\n",
    "# Split into train/val\n",
    "train_set_improved, val_set_improved = random_split(improved_dataset, [0.85, 0.15])\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE_IMPROVED = 32\n",
    "\n",
    "train_loader_improved = DataLoader(\n",
    "    train_set_improved,\n",
    "    batch_size=BATCH_SIZE_IMPROVED,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_improved\n",
    ")\n",
    "\n",
    "val_loader_improved = DataLoader(\n",
    "    val_set_improved,\n",
    "    batch_size=BATCH_SIZE_IMPROVED,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_improved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30d371ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training improved model...\n",
      "Epoch 1/10\n",
      "  Train Loss: 0.5575, Train Acc: 0.7086\n",
      "  Val Loss: 0.4389, Val Acc: 0.8028\n",
      "  ✓ New best model saved! (Val Acc: 0.8028)\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.3668, Train Acc: 0.8514\n",
      "  Val Loss: 0.4568, Val Acc: 0.8019\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.2765, Train Acc: 0.8971\n",
      "  Val Loss: 0.4621, Val Acc: 0.7809\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.1890, Train Acc: 0.9356\n",
      "  Val Loss: 0.6138, Val Acc: 0.7862\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.1378, Train Acc: 0.9527\n",
      "  Val Loss: 0.6761, Val Acc: 0.7748\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.1047, Train Acc: 0.9631\n",
      "  Val Loss: 0.8079, Val Acc: 0.7713\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.0677, Train Acc: 0.9742\n",
      "  Val Loss: 0.9675, Val Acc: 0.7713\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.0575, Train Acc: 0.9793\n",
      "  Val Loss: 1.0492, Val Acc: 0.7695\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.0488, Train Acc: 0.9816\n",
      "  Val Loss: 1.2112, Val Acc: 0.7669\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.0397, Train Acc: 0.9842\n",
      "  Val Loss: 1.3019, Val Acc: 0.7651\n",
      "\n",
      "Best validation accuracy: 0.8028\n"
     ]
    }
   ],
   "source": [
    "# Train improved model with better hyperparameters\n",
    "\n",
    "# Initialize model\n",
    "improved_model = ImprovedModel(\n",
    "    vocab_size=len(improved_dataset.text_vocab),\n",
    "    keyword_size=len(improved_dataset.keyword_vocab),\n",
    "    location_size=len(improved_dataset.location_vocab),\n",
    "    embed_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_class=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# Better optimizer and scheduler\n",
    "optimizer_improved = optim.Adam(improved_model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_improved, step_size=3, gamma=0.5)\n",
    "criterion_improved = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with validation monitoring\n",
    "NUM_EPOCHS = 10\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"Training improved model...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_improved(\n",
    "        improved_model, train_loader_improved, \n",
    "        optimizer_improved, criterion_improved, scheduler\n",
    "    )\n",
    "    val_loss, val_acc = evaluate_improved(\n",
    "        improved_model, val_loader_improved, criterion_improved\n",
    "    )\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(improved_model.state_dict(), 'best_model.pt')\n",
    "        print(f\"  ✓ New best model saved! (Val Acc: {val_acc:.4f})\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c927a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved submission saved! Shape: (3263, 2)\n",
      "\n",
      "Prediction distribution:\n",
      "target\n",
      "0    2201\n",
      "1    1062\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate submission with improved model\n",
    "\n",
    "# Load best model\n",
    "improved_model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Create test dataset using training vocabs\n",
    "test_dataset_improved = ImprovedTweetsDataset(\n",
    "    test_df,\n",
    "    keyword_vocab=improved_dataset.keyword_vocab,\n",
    "    location_vocab=improved_dataset.location_vocab,\n",
    "    text_vocab=improved_dataset.text_vocab,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader_improved = DataLoader(\n",
    "    test_dataset_improved,\n",
    "    batch_size=BATCH_SIZE_IMPROVED,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_improved\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "predictions_improved = predict_improved(improved_model, test_loader_improved)\n",
    "\n",
    "# Create submission\n",
    "submission_improved = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': predictions_improved\n",
    "})\n",
    "\n",
    "submission_improved.to_csv('submission_improved.csv', index=False)\n",
    "print(f\"Improved submission saved! Shape: {submission_improved.shape}\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission_improved['target'].value_counts())\n",
    "submission_improved.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf4b03",
   "metadata": {},
   "source": [
    "## Summary of Improvements\n",
    "\n",
    "### What Changed from Baseline (65% accuracy):\n",
    "\n",
    "**1. Multi-Input Features** ✓\n",
    "- Added **keyword** embeddings (32-dim)\n",
    "- Added **location** embeddings (32-dim)\n",
    "- Handles missing values with `<missing>` token\n",
    "\n",
    "**2. Better Architecture** ✓\n",
    "- **Bidirectional LSTM** (2 layers, 128 hidden units) instead of EmbeddingBag\n",
    "- Captures word order and context (critical for \"not bad\" vs \"bad\")\n",
    "- Dropout layers (0.3) to prevent overfitting\n",
    "- Combined feature representation: LSTM output + keyword + location\n",
    "\n",
    "**3. Training Improvements** ✓\n",
    "- **Adam optimizer** (lr=0.001) instead of SGD\n",
    "- **Learning rate scheduler** (decay every 3 epochs)\n",
    "- **10 epochs** with validation monitoring\n",
    "- **Gradient clipping** (prevents exploding gradients)\n",
    "- **Model checkpointing** (saves best model based on validation)\n",
    "\n",
    "**4. Data Improvements** ✓\n",
    "- Text lowercasing for consistency\n",
    "- Padded batches (variable length sequences)\n",
    "- Larger batch size (32 vs 1-16)\n",
    "\n",
    "### Expected Results:\n",
    "- Baseline: ~65% (bag-of-words, text only)\n",
    "- **Improved: 75-82%** (typical for LSTM + multi-features on this dataset)\n",
    "\n",
    "### Next Steps to Push Further (if needed):\n",
    "1. **Pre-trained embeddings**: Use GloVe or Word2Vec instead of random init\n",
    "2. **Advanced models**: BERT, RoBERTa (transformers) - can reach 85%+\n",
    "3. **Text preprocessing**: Remove URLs, handle hashtags, expand contractions\n",
    "4. **Ensemble**: Combine multiple models\n",
    "5. **Feature engineering**: Tweet length, punctuation count, capitalization patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c947b",
   "metadata": {},
   "source": [
    "## BERT Implementation (State-of-the-Art)\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained transformer model that achieves excellent results on text classification tasks.\n",
    "\n",
    "### Why BERT?\n",
    "- **Pre-trained** on massive text corpus (understands language context)\n",
    "- **Bidirectional**: Reads text in both directions simultaneously\n",
    "- **Transfer learning**: Fine-tune on your specific task with less data\n",
    "- **Expected accuracy**: 82-88% on this dataset\n",
    "\n",
    "### Requirements:\n",
    "Install transformers library: `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70fe8f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Transformers version: 4.57.1\n",
      "✓ PyTorch version: 2.9.0+cu128\n"
     ]
    }
   ],
   "source": [
    "# Import BERT components\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if transformers is installed\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"✓ Transformers version: {transformers.__version__}\")\n",
    "    print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"⚠ Please install transformers: pip install transformers\")\n",
    "    print(\"  Or run: !pip install transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe708d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTweetsDataset(Dataset):\n",
    "    \"\"\"Dataset for BERT with tokenization using BertTokenizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_length=128, is_test=False):\n",
    "        self.is_test = is_test\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Text data\n",
    "        self.texts = df['text'].fillna('').tolist()\n",
    "        \n",
    "        # Optional: incorporate keyword and location into text\n",
    "        # This helps BERT understand context better\n",
    "        self.keywords = df['keyword'].fillna('').tolist()\n",
    "        self.locations = df['location'].fillna('').tolist()\n",
    "        \n",
    "        # Concatenate keyword/location with text for richer context\n",
    "        self.full_texts = []\n",
    "        for i in range(len(self.texts)):\n",
    "            # Format: \"[keyword] [location] text\"\n",
    "            parts = []\n",
    "            if self.keywords[i]:\n",
    "                parts.append(f\"[{self.keywords[i]}]\")\n",
    "            if self.locations[i]:\n",
    "                parts.append(f\"[{self.locations[i]}]\")\n",
    "            parts.append(self.texts[i])\n",
    "            self.full_texts.append(' '.join(parts))\n",
    "        \n",
    "        # Labels\n",
    "        if not is_test:\n",
    "            self.labels = df['target'].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.full_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.full_texts[idx]\n",
    "        \n",
    "        # BERT tokenization\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        label = self.labels[idx] if not self.is_test else 0\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9752233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 6471\n",
      "Val samples: 1142\n",
      "\n",
      "Sample encoding:\n",
      "Input IDs shape: torch.Size([128])\n",
      "Attention mask shape: torch.Size([128])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT tokenizer and create datasets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create BERT datasets\n",
    "bert_dataset = BertTweetsDataset(train_df, tokenizer, max_length=128)\n",
    "\n",
    "# Split into train/val\n",
    "from torch.utils.data import random_split\n",
    "train_size = int(0.85 * len(bert_dataset))\n",
    "val_size = len(bert_dataset) - train_size\n",
    "train_bert, val_bert = random_split(bert_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train samples: {len(train_bert)}\")\n",
    "print(f\"Val samples: {len(val_bert)}\")\n",
    "print(f\"\\nSample encoding:\")\n",
    "sample = bert_dataset[0]\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4a7bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    \"\"\"Train BERT for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        total_acc += (preds == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(dataloader), total_acc / total_count\n",
    "\n",
    "\n",
    "def evaluate_bert(model, dataloader, device):\n",
    "    \"\"\"Evaluate BERT model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_acc += (preds == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(dataloader), total_acc / total_count\n",
    "\n",
    "\n",
    "def predict_bert(model, dataloader, device):\n",
    "    \"\"\"Generate predictions with BERT.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3afa061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model loaded: bert\n",
      "Total parameters: 109,483,778\n",
      "Training for 3 epochs with batch size 16\n",
      "Total training steps: 1215\n",
      "\n",
      "Starting training...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(BERT_EPOCHS):\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBERT_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     train_loss, train_acc = \u001b[43mtrain_bert_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbert_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_bert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_bert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_bert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     val_loss, val_acc = evaluate_bert(bert_model, val_loader_bert, device)\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtrain_bert_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, scheduler, device)\u001b[39m\n\u001b[32m     24\u001b[39m loss.backward()\n\u001b[32m     25\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m scheduler.step()\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/THU/ML/Project/.venv/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:133\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m opt = opt_ref()\n\u001b[32m    132\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/THU/ML/Project/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/THU/ML/Project/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/THU/ML/Project/.venv/lib/python3.13/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/THU/ML/Project/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/THU/ML/Project/.venv/lib/python3.13/site-packages/torch/optim/adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/THU/ML/Project/.venv/lib/python3.13/site-packages/torch/optim/adam.py:537\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    535\u001b[39m         denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m     \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch.is_complex(params[i]):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training BERT model\n",
    "\n",
    "BERT_BATCH_SIZE = 16  # Smaller batch for BERT (memory intensive)\n",
    "BERT_EPOCHS = 3  # 3-4 epochs usually enough for fine-tuning\n",
    "BERT_LR = 2e-5  # Standard learning rate for BERT fine-tuning\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader_bert = DataLoader(train_bert, batch_size=BERT_BATCH_SIZE, shuffle=True)\n",
    "val_loader_bert = DataLoader(val_bert, batch_size=BERT_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize BERT model for classification\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ").to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer_bert = AdamW(bert_model.parameters(), lr=BERT_LR, eps=1e-8)\n",
    "\n",
    "total_steps = len(train_loader_bert) * BERT_EPOCHS\n",
    "scheduler_bert = get_linear_schedule_with_warmup(\n",
    "    optimizer_bert,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"BERT Model loaded: {bert_model.config.model_type}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in bert_model.parameters()):,}\")\n",
    "print(f\"Training for {BERT_EPOCHS} epochs with batch size {BERT_BATCH_SIZE}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"\\nStarting training...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "best_val_acc_bert = 0\n",
    "\n",
    "for epoch in range(BERT_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{BERT_EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc = train_bert_epoch(\n",
    "        bert_model, train_loader_bert, optimizer_bert, scheduler_bert, device\n",
    "    )\n",
    "    val_loss, val_acc = evaluate_bert(bert_model, val_loader_bert, device)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc_bert:\n",
    "        best_val_acc_bert = val_acc\n",
    "        torch.save(bert_model.state_dict(), 'best_bert_model.pt')\n",
    "        print(f\"  ✓ New best BERT model saved! (Val Acc: {val_acc:.4f})\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc_bert:.4f}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48873d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate BERT predictions for submission\n",
    "\n",
    "# Load best model\n",
    "bert_model.load_state_dict(torch.load('best_bert_model.pt'))\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset_bert = BertTweetsDataset(test_df, tokenizer, max_length=128, is_test=True)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, batch_size=BERT_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Generate predictions\n",
    "predictions_bert = predict_bert(bert_model, test_loader_bert, device)\n",
    "\n",
    "# Create submission\n",
    "submission_bert = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': predictions_bert\n",
    "})\n",
    "\n",
    "submission_bert.to_csv('submission_bert.csv', index=False)\n",
    "\n",
    "print(f\"✓ BERT submission saved to: submission_bert.csv\")\n",
    "print(f\"Shape: {submission_bert.shape}\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission_bert['target'].value_counts())\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "submission_bert.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e462b",
   "metadata": {},
   "source": [
    "## BERT Implementation Summary\n",
    "\n",
    "### What BERT does differently:\n",
    "\n",
    "**1. Pre-trained Understanding**\n",
    "- Trained on 3.3 billion words (Wikipedia + BookCorpus)\n",
    "- Already understands language context, grammar, semantics\n",
    "- We just fine-tune it for disaster classification\n",
    "\n",
    "**2. Attention Mechanism**\n",
    "- Looks at all words simultaneously\n",
    "- Understands relationships between distant words\n",
    "- Example: \"fire\" and \"building\" together → disaster\n",
    "\n",
    "**3. Contextual Embeddings**\n",
    "- Word meaning changes based on context\n",
    "- \"bank\" in \"river bank\" vs \"financial bank\"\n",
    "- BERT understands this distinction\n",
    "\n",
    "**4. Feature Integration**\n",
    "- We concatenate keyword/location with text: `[keyword] [location] text`\n",
    "- BERT processes everything together\n",
    "- No need for separate embeddings\n",
    "\n",
    "### Expected Performance:\n",
    "\n",
    "| Model | Validation Acc | Test Acc (expected) |\n",
    "|-------|----------------|---------------------|\n",
    "| Baseline (EmbeddingBag) | 65% | 65% |\n",
    "| LSTM + Features | 75-78% | 73-76% |\n",
    "| **BERT** | **82-86%** | **80-85%** |\n",
    "\n",
    "### Tips for Better BERT Results:\n",
    "\n",
    "1. **Increase epochs** (if not overfitting): 4-5 epochs\n",
    "2. **Adjust batch size**: Larger if you have GPU memory\n",
    "3. **Try different BERT variants**:\n",
    "   - `distilbert-base-uncased` - faster, 97% of BERT performance\n",
    "   - `roberta-base` - often better than BERT\n",
    "   - `bert-large-uncased` - more parameters, better accuracy\n",
    "4. **Text preprocessing**: Remove URLs, normalize hashtags\n",
    "5. **Ensemble**: Combine BERT + LSTM predictions\n",
    "\n",
    "### Memory & Speed:\n",
    "- BERT is memory-intensive (110M parameters)\n",
    "- CPU: ~5-10 min/epoch\n",
    "- GPU (recommended): ~30-60 sec/epoch\n",
    "- Reduce batch size if you get OOM errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
